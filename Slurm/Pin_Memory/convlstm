#!/bin/bash

#SBATCH --job-name="SC6-pin_memorry-convlstm"

#SBATCH --nodes=1

#SBATCH --gpus-per-node=rtx8000:1 # specify gpu

#SBATCH --ntasks-per-node=1

#SBATCH --cpus-per-task=4       # cpu-cores per task (>1 if multi-threaded tasks)

#SBATCH --mem-per-cpu=16G               # memory per node (4G per cpu-core is default)

#SBATCH --time=48:00:00                                  # set runtime

#SBATCH -o /home/mila/f/felix-andreas.nahrstedt/Slurm/SC6-pin_memorry-convlstm.out        # set log dir to home


EXP_NAME=$1
DL_FRAMEWORK="torch"

echo "Beginning experiment $EXP_NAME."

# 1. Load Python

module load python/3.10
export PYTHONPATH=$(pwd)
echo $PYTHONPATH

# 2. Load DL Framework

if [[ $DL_FRAMEWORK == "torch" ]]; then

    module load cuda/10.0/cudnn/7.6
    #module load python/3.7/cuda/11.1/cudnn/8.0/pytorch/1.8.1
    
fi

# 3. Create or Set Up Environment
deactivate

if [ -a env_new_emulator/bin/activate ]; then

    source env_new_emulator/bin/activate
    echo "activated"

else
    python -m venv env_new_emulator
    source env_new_emulator/bin/activate
    #bash download_climateset.sh || { echo "Failed to run download_climateset.sh"; exit 1; }
    pip install -r requirements.txt || { echo "Failed to install requirements."; exit 1; }
    pip install pytorch-lightning==2.2.2 || { echo "Failed to install pytorch-lightning old version"; exit 1; }
fi

echo $PYTHONPATH
dir
cd $(pwd)


export NCCL_BLOCKING_WAIT=1 #Pytorch Lightning uses the NCCL backend for inter-GPU communication by default. Set this variable to avoid timeout errors.


# 8. Run Python
export HYDRA_FULL_ERROR=1
echo "Running python test.py ..."
srun  python emulator/run.py experiment=Scaling_Experiments/Pin_Memory/superemulator_convlstm_tas+pr_run-02.yaml seed=1234


# 9. Copy output to scratch
#cp /home/mila/f/felix-andreas.nahrstedt/Slurm/SC6-pin_memorry-convlstm_out.out 

# 10. Experiment is finished

echo "Experiment $EXP_NAME is concluded."