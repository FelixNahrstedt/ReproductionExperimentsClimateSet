#!/bin/bash

#SBATCH --job-name="NW0-SC6-RTX8000-basetest-unet"

#SBATCH --nodes=1

#SBATCH --gpus-per-node=a100:1 --constraint="dgx&ampere"       # specify gpu

#SBATCH --ntasks-per-node=1

#SBATCH --cpus-per-task=4       # cpu-cores per task (>1 if multi-threaded tasks)

#SBATCH --mem-per-cpu=16G               # memory per node (4G per cpu-core is default)

#SBATCH --time=03:30:00                                  # set runtime

#SBATCH -o /home/mila/f/felix-andreas.nahrstedt/Slurm/NW0-SC6-RTX8000-basetest-unet.out        # set log dir to home

module load python/3.10
export PYTHONPATH=$(pwd)




# 3. Create or Set Up Environment

source env_new_emulator/bin/activate
echo "activated"

echo $PYTHONPATH
dir
cd $(pwd)


export NCCL_BLOCKING_WAIT=1 #Pytorch Lightning uses the NCCL backend for inter-GPU communication by default. Set this variable to avoid timeout errors.

# Get a unique port for this job based on the job ID
# export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))
# export MASTER_ADDR="127.0.0.1"
rsync /home/mila/f/felix-andreas.nahrstedt/scratch/Climateset_DATA $SLURM_TMPDIR
# 8. Run Python
export HYDRA_FULL_ERROR=1
echo "Running python test.py ..."
srun python emulator/run.py experiment=Scaling_Experiments/Num_Workers/convlstm/superemulator_NW0.yaml  seed=3423 # seed=1234


# 9. Copy output to scratch
#cp /home/mila/f/felix-andreas.nahrstedt/Slurm/SC6-RTX8000-basetest-unet_out.out 

# 10. Experiment is finished

echo "Experiment $EXP_NAME is concluded."